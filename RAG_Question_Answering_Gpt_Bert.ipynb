{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN12zVtpdYYGj0Qy+jh3ffi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junya17/RAG-Question-Answering-Gpt-Bert/blob/main/RAG_Question_Answering_Gpt_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "gEd9FvAYVsbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertModel\n",
        "\n",
        "# GPT-2モデルとトークナイザーのロード\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# BERTモデルとトークナイザーのロード\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_sentence_vector(sentence, model, tokenizer):\n",
        "    \"\"\"文のBERTベクトルを取得する関数\"\"\"\n",
        "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, max_length=128)\n",
        "    outputs = model(**inputs)\n",
        "    sentence_vector = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "    return sentence_vector.reshape(-1)\n",
        "\n",
        "def search_closest_question(query, index, qa_pairs, model, tokenizer):\n",
        "    \"\"\"FAISSインデックスを使用して、質問に最も近いQAペアを検索する関数\"\"\"\n",
        "    query_vector = get_sentence_vector(query, model, tokenizer)\n",
        "    _, I = index.search(np.array([query_vector]), k=1)\n",
        "    return qa_pairs[I[0][0]]\n",
        "\n",
        "def generate_answer_with_gpt(document, query, tokenizer, model, max_length=300, temperature=1.0, top_k=50, top_p=0.95):\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    combined_input = query + \" \" + document\n",
        "    inputs = tokenizer.encode_plus(combined_input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        do_sample=True  # この行を追加\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "def extract_answer(text, query):\n",
        "    # 質問の後の部分を見つける\n",
        "    answer_start = text.find(query) + len(query)\n",
        "    if answer_start > -1:\n",
        "        # 質問の後のテキストを抽出\n",
        "        answer_text = text[answer_start:].strip()\n",
        "        # 最初のピリオドまでのテキストを回答として返す\n",
        "        answer_end = answer_text.find(\".\")\n",
        "        if answer_end > -1:\n",
        "            answer_text = answer_text[:answer_end]\n",
        "\n",
        "        # # 不要な繰り返しを除去するための追加ロジック\n",
        "        # # 例: 特定の繰り返しパターンを探して削除\n",
        "        # repeat_pattern = \"私は、その言うです。\"\n",
        "        # if repeat_pattern in answer_text:\n",
        "        #     answer_text = answer_text.split(repeat_pattern)[0]\n",
        "\n",
        "        return answer_text\n",
        "    else:\n",
        "        return \"回答が見つかりませんでした。\"\n",
        "\n",
        "# QAペアとベクトルデータベースの準備\n",
        "qa_pairs = [\n",
        "    {\n",
        "        \"question\": \"東京の名所はどこですか？\",\n",
        "        \"answer\": \"浅草寺、スカイツリー、渋谷の交差点\",\n",
        "        \"documents\": [\n",
        "            \"浅草寺は東京の有名な歴史的寺院です。\",\n",
        "            \"東京スカイツリーは高さ634メートルのタワーで、展望台からの景色が素晴らしい。\",\n",
        "            \"渋谷の交差点は、世界でも有名な繁忙な交差点です。\"\n",
        "        ]\n",
        "    }\n",
        "    # ここに追加のQAペアを追加可能\n",
        "]\n",
        "\n",
        "vectors = [get_sentence_vector(pair['question'], bert_model, bert_tokenizer) for pair in qa_pairs]\n",
        "dim = vectors[0].shape[0]\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(np.array(vectors))\n",
        "\n",
        "# メイン処理\n",
        "user_question = \"東京の名所はどこですか？\"\n",
        "closest_pair = search_closest_question(user_question, index, qa_pairs, bert_model, bert_tokenizer)\n",
        "document_text = closest_pair[\"answer\"]\n",
        "generated_answer = generate_answer_with_gpt(document_text, user_question, gpt_tokenizer, gpt_model)\n",
        "final_answer = extract_answer(generated_answer, user_question)\n",
        "\n",
        "print(\"Final answer:\", final_answer)\n"
      ],
      "metadata": {
        "id": "CKDadVSUgox5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}